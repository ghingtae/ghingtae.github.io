---
layout: single
title:  "DACON: ëª¨ë¸ ì–‘ìí™” + RAG - NLP(ìì—°ì–´ ì²˜ë¦¬) ê¸°ë°˜ì˜ QA(ì§ˆë¬¸-ì‘ë‹µ) ì‹œìŠ¤í…œ ê°œë°œ"
categories: Dacon
tag: [coding, Colab, QA, RAG, Quantization, LLM, NLP]
toc: true
toc_sticky: true
author_profile: true
---

<head>
  <style>
    table.dataframe {
      white-space: normal;
      width: 100%;
      height: 240px;
      display: block;
      overflow: auto;
      font-family: Arial, sans-serif;
      font-size: 0.9rem;
      line-height: 20px;
      text-align: center;
      border: 0px !important;
    }

    table.dataframe th {
      text-align: center;
      font-weight: bold;
      padding: 8px;
    }

    table.dataframe td {
      text-align: center;
      padding: 8px;
    }

    table.dataframe tr:hover {
      background: #b8d1f3; 
    }

    .output_prompt {
      overflow: auto;
      font-size: 0.9rem;
      line-height: 1.45;
      border-radius: 0.3rem;
      -webkit-overflow-scrolling: touch;
      padding: 0.8rem;
      margin-top: 0;
      margin-bottom: 15px;
      font: 1rem Consolas, "Liberation Mono", Menlo, Courier, monospace;
      color: $code-text-color;
      border: solid 1px $border-color;
      border-radius: 0.3rem;
      word-break: normal;
      white-space: pre;
    }

  .dataframe tbody tr th:only-of-type {
      vertical-align: middle;
  }

  .dataframe tbody tr th {
      vertical-align: top;
  }

  .dataframe thead th {
      text-align: center !important;
      padding: 8px;
  }

  .page__content p {
      margin: 0 0 0px !important;
  }

  .page__content p > strong {
    font-size: 1rem !important;
  }

  </style>
</head>   


## Content Summary



ë³¸ íŠœí† ë¦¬ì–¼ì€ ë„ë°° í•˜ìì™€ ê´€ë ¨ëœ ê¹Šì´ ìˆëŠ” ì§ˆì˜ì‘ë‹µ ì²˜ë¦¬ ëŠ¥ë ¥ì„ ê°–ì¶˜ AI ëª¨ë¸ì„ ê°œë°œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.   

**ëª¨ë¸ ì••ì¶•(ì–‘ìí™”)** ì„ í†µí•´ ëª¨ë¸ ì„±ëŠ¥ì€ ìœ ì§€í•˜ê³  GPUì— ì˜¬ë¼ê°€ëŠ” ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ë¥¼ ì¤„ì´ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.   

ë˜í•œ, ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì´ ë¹ ë¥´ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆê²Œ **R.A.G(ê²€ìƒ‰, ì¦ê°•, ìƒì„±)** ì„ ì‚¬ìš©í•˜ì—¬ ê°€ë²¼ìš´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’íˆëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.


### Modeling Process



#### 1. **ëª¨ë¸ ì–‘ìí™”(Quantization)**   

![ëª¨ë¸ ì–‘ìí™”](/assets/images/Quantization.png)      

   

#### 2. **R.A.G(ê²€ìƒ‰, ì¦ê°•, ìƒì„±)**   

![RAG ë¬¸ì„œ ë¡œë“œ ë° Vector DB ì €ì¥](/assets/images/langchain_indexing.png)   



ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì¸ë±ì‹±í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.   

1. **ë¡œë“œ**: ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ `DocumentLoader`ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

2. **ë¶„í• **: `Text splitters`ëŠ” í° `Documents`ë¥¼ ì¢€ ë” ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. ì´ê²ƒì€ ë°ì´í„°ë¥¼ ì¸ë±ì‹±í•˜ê³  ëª¨ë¸ì— ì „ë‹¬í•˜ëŠ” ê²ƒì— ìœ ìš©í•˜ë©°, í° ì²­í¬ëŠ” ê²€ìƒ‰í•˜ê¸° ì–´ë µê³  ëª¨ë¸ì˜ ìœ í•œí•œ ì»¨í…ìŠ¤íŠ¸ ì°½ì— ë§ì§€ ì•ŠìŠµë‹ˆë‹¤.

3. **ì„ë² ë”©**: ë¬¸ì„œë¥¼ ë²¡í„° í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

4. **ì €ì¥(ë²¡í„°DB)**: ë‚˜ì¤‘ì— ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ ë¶„í• ì„ ì €ì¥í•˜ê³  ì¸ë±ì‹±í•  ì¥ì†Œê°€ í•„ìš”í•©ë‹ˆë‹¤. ì´ëŠ” ì¢…ì¢… `VectorStore`ì™€ `Embeddings`ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰ë©ë‹ˆë‹¤.   



![RAG ë¬¸ì„œ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„±](/assets/images/langchain_Q_A.png)   



1. **ê²€ìƒ‰**: ì‚¬ìš©ì ì…ë ¥ì´ ì£¼ì–´ì§€ë©´ `Retrive`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ì¥ì†Œì—ì„œ ê´€ë ¨ ë¶„í• ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤.

2. **í”„ë¡¬í”„íŠ¸**: ê²€ìƒ‰í•œ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ë„ì¶œí•˜ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

3. **ëª¨ë¸**: ëª¨ë¸(ChatModel, LLM etc)ì„ ì„ íƒí•©ë‹ˆë‹¤.

4. **ìƒì„±**: `ChatModel / LLM`ì€ ì§ˆë¬¸ê³¼ ê²€ìƒ‰ëœ ë°ì´í„°ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.


## Colab Drive



```python
from google.colab import drive
drive.mount('/content/drive')
```

<pre>
Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount("/content/drive", force_remount=True).
</pre>
## Install



```python
# accelerate ì„¤ì¹˜ ì˜¤ë¥˜ì‹œ ì‹¤í–‰
# !pip3 install -q -U git+https://github.com/huggingface/accelerate.git
# import os
# from accelerate.utils import write_basic_config

# write_basic_config()  # Write a config file
# os._exit(00)  # Restart the notebook
```


```python
# pip installì‹œ utf-8, ansi ê´€ë ¨ ì˜¤ë¥˜ë‚  ê²½ìš° í•„ìš”í•œ ì½”ë“œ
import locale
def getpreferredencoding(do_setlocale = True):
    return "UTF-8"
locale.getpreferredencoding = getpreferredencoding
```


```python
!pip -q install pypdf chromadb sentence-transformers faiss-gpu

# LLM ì–‘ìí™”ì— í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
!pip3 install -q -U bitsandbytes
!pip3 install -q -U git+https://github.com/huggingface/transformers.git
!pip3 install -q -U git+https://github.com/huggingface/peft.git
!pip3 install -q -U git+https://github.com/huggingface/accelerate.git

# RAG í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
!pip install openai langchain
!pip install huggingface_hub trainsformers datasets
!pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4
```

<pre>
[?25l     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m0.0/286.1 kB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[91mâ•¸[0m[90mâ”[0m [32m276.5/286.1 kB[0m [31m8.5 MB/s[0m eta [36m0:00:01[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m286.1/286.1 kB[0m [31m6.7 MB/s[0m eta [36m0:00:00[0m
[?25h[?25l     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m0.0/525.5 kB[0m [31m?[0m eta [36m-:--:--[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m525.5/525.5 kB[0m [31m45.6 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m156.5/156.5 kB[0m [31m17.5 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m85.5/85.5 MB[0m [31m7.9 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m2.4/2.4 MB[0m [31m67.8 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m92.1/92.1 kB[0m [31m10.3 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m60.8/60.8 kB[0m [31m7.8 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m41.3/41.3 kB[0m [31m5.7 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m5.4/5.4 MB[0m [31m65.5 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m6.8/6.8 MB[0m [31m47.1 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m58.4/58.4 kB[0m [31m8.7 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m105.7/105.7 kB[0m [31m12.3 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m67.3/67.3 kB[0m [31m10.1 MB/s[0m eta [36m0:00:00[0m
[?25h  Installing build dependencies ... [?25l[?25hdone
  Getting requirements to build wheel ... [?25l[?25hdone
  Preparing metadata (pyproject.toml) ... [?25l[?25hdone
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m698.9/698.9 kB[0m [31m56.4 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1.6/1.6 MB[0m [31m42.1 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m67.6/67.6 kB[0m [31m9.7 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m138.5/138.5 kB[0m [31m12.8 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m71.5/71.5 kB[0m [31m10.1 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m46.0/46.0 kB[0m [31m6.1 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m50.8/50.8 kB[0m [31m7.3 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m58.3/58.3 kB[0m [31m8.9 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m341.4/341.4 kB[0m [31m29.9 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m3.4/3.4 MB[0m [31m73.7 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1.3/1.3 MB[0m [31m65.7 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m130.2/130.2 kB[0m [31m14.1 MB/s[0m eta [36m0:00:00[0m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m86.8/86.8 kB[0m [31m12.1 MB/s[0m eta [36m0:00:00[0m
[?25h  Building wheel for pypika (pyproject.toml) ... [?25l[?25hdone
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m102.2/102.2 MB[0m [31m9.4 MB/s[0m eta [36m0:00:00[0m
[?25h  Installing build dependencies ... [?25l[?25hdone
  Getting requirements to build wheel ... [?25l[?25hdone
  Preparing metadata (pyproject.toml) ... [?25l[?25hdone
  Building wheel for transformers (pyproject.toml) ... [?25l[?25hdone
  Installing build dependencies ... [?25l[?25hdone
  Getting requirements to build wheel ... [?25l[?25hdone
  Preparing metadata (pyproject.toml) ... [?25l[?25hdone
  Building wheel for peft (pyproject.toml) ... [?25l[?25hdone
  Installing build dependencies ... [?25l[?25hdone
  Getting requirements to build wheel ... [?25l[?25hdone
  Preparing metadata (pyproject.toml) ... [?25l[?25hdone
Collecting openai
  Downloading openai-1.14.2-py3-none-any.whl (262 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m262.4/262.4 kB[0m [31m4.4 MB/s[0m eta [36m0:00:00[0m
[?25hCollecting langchain
  Downloading langchain-0.1.13-py3-none-any.whl (810 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m810.5/810.5 kB[0m [31m48.8 MB/s[0m eta [36m0:00:00[0m
[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)
Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)
Collecting httpx<1,>=0.23.0 (from openai)
  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m75.6/75.6 kB[0m [31m10.8 MB/s[0m eta [36m0:00:00[0m
[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)
Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)
Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)
Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)
Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)
Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)
Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)
Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)
Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)
  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)
Collecting jsonpatch<2.0,>=1.33 (from langchain)
  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)
Collecting langchain-community<0.1,>=0.0.29 (from langchain)
  Downloading langchain_community-0.0.29-py3-none-any.whl (1.8 MB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1.8/1.8 MB[0m [31m64.8 MB/s[0m eta [36m0:00:00[0m
[?25hCollecting langchain-core<0.2.0,>=0.1.33 (from langchain)
  Downloading langchain_core-0.1.33-py3-none-any.whl (269 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m269.1/269.1 kB[0m [31m29.0 MB/s[0m eta [36m0:00:00[0m
[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)
  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)
Collecting langsmith<0.2.0,>=0.1.17 (from langchain)
  Downloading langsmith-0.1.31-py3-none-any.whl (71 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m71.6/71.6 kB[0m [31m8.2 MB/s[0m eta [36m0:00:00[0m
[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)
Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)
Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)
Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)
Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)
Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)
Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)
Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)
Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)
Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)
Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)
  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m49.4/49.4 kB[0m [31m5.8 MB/s[0m eta [36m0:00:00[0m
[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)
  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)
Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)
Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)
  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m77.8/77.8 kB[0m [31m10.4 MB/s[0m eta [36m0:00:00[0m
[?25hRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)
Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)
  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)
Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.33->langchain)
  Downloading packaging-23.2-py3-none-any.whl (53 kB)
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m53.0/53.0 kB[0m [31m7.6 MB/s[0m eta [36m0:00:00[0m
[?25hRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)
Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)
Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)
Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)
Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)
Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)
Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)
  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)
Installing collected packages: packaging, mypy-extensions, jsonpointer, httpcore, typing-inspect, marshmallow, jsonpatch, httpx, openai, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain
  Attempting uninstall: packaging
    Found existing installation: packaging 24.0
    Uninstalling packaging-24.0:
      Successfully uninstalled packaging-24.0
Successfully installed dataclasses-json-0.6.4 httpcore-1.0.4 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.13 langchain-community-0.0.29 langchain-core-0.1.33 langchain-text-splitters-0.0.1 langsmith-0.1.31 marshmallow-3.21.1 mypy-extensions-1.0.0 openai-1.14.2 packaging-23.2 typing-inspect-0.9.0
Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)
[31mERROR: Could not find a version that satisfies the requirement trainsformers (from versions: none)[0m[31m
[0m[31mERROR: No matching distribution found for trainsformers[0m[31m
[2K     [90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [32m1.8/1.8 MB[0m [31m37.6 MB/s[0m eta [36m0:00:00[0m
[?25h
</pre>
## Import



```python
import warnings # ê²½ê³  ë¬´ì‹œ
import pandas as pd
from tqdm.auto import tqdm # ì§„í–‰ìƒí™© í‘œì‹œ
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
import os
import bs4 # ì›¹ í˜ì´ì§€ë¥¼ íŒŒì‹±í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain import hub # AIì™€ ê´€ë ¨ëœ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain.text_splitter import RecursiveCharacterTextSplitter # í…ìŠ¤íŠ¸ ë¶„í• 
from langchain_community.document_loaders import WebBaseLoader, PyPDFLoader # ë¬¸ì„œ ë¡œë”©ê³¼ PDF ë¡œë”©
from langchain_community.vectorstores import Chroma,FAISS # ë²¡í„° ì €ì¥
from langchain.vectorstores import FAISS # ë²¡í„° ì €ì¥
from langchain_core.output_parsers import StrOutputParser # ì¶œë ¥ íŒŒì‹±
from langchain_core.runnables import RunnablePassthrough # ì‹¤í–‰ ê°€ëŠ¥í•œ íŒ¨ìŠ¤ìŠ¤ë£¨
from langchain.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
from langchain.embeddings.huggingface import HuggingFaceEmbeddings # í—ˆê¹…í˜ì´ìŠ¤ ì„ë² ë”©
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import LLMChain
from langchain.document_loaders.csv_loader import CSVLoader # csv ë¡œë”©
from langchain.document_loaders import PyPDFLoader # pdf ë¡œë”©
```

## íŠ¸ëœìŠ¤í¬ë¨¸ì—ì„œ BitsandBytesConfigë¥¼ í†µí•´ ì–‘ìí™” ë§¤ê°œë³€ìˆ˜ ì •ì˜í•˜ê¸°



```python
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True, # ëª¨ë¸ì„ 4ë¹„íŠ¸ ì •ë°€ë„ë¡œ ë³€í™˜í•˜ê³  ë¡œë“œí•˜ë„ë¡ ì§€ì •í•œë‹¤.
    bnb_4bit_use_double_quant=True, # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ë†’íˆê¸° ìœ„í•´ ì¤‘ì²© ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡  ë° í•™ìŠµí•œë‹¤.
    bnb_4bit_quant_type="nf4", # 4ë¹„íŠ¸ í†µí•©ì—ëŠ” 2ê°€ì§€ ì–‘ìí™” ìœ í˜•ì¸ FP4ì™€ NF4ê°€ ì œê³µëœë‹¤. NF4 dtypeì€ Normal Float 4ë¥¼ ë‚˜íƒ€ë‚´ë©°, QLoRA ë°±ì„œì— ì†Œê°œ ë˜ì–´ìˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ FP4 ì–‘ìí™”ë¥¼ ì‚¬ìš©í•œë‹¤.
    bnb_4bit_compute_dtype=torch.bfloat16 # ê³„ì‚° ì¤‘ ì‚¬ìš©í•  dtypeì„ ë³€ê²½í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ê³„ì‚° dtype. ê¸°ë³¸ì ìœ¼ë¡œ ê³„ì‚° dtypeì€ float32ë¡œ ì„¤ì •ë˜ì–´ ìˆì§€ë§Œ ê³„ì‚° ì†ë„ë¥¼ ë†’íˆê¸° ìœ„í•´ bf16ìœ¼ë¡œ ì„¤ì • ê°€ëŠ¥í•˜ë‹¤.
)
```

* **load_in_4bit = True:** ëª¨ë¸ì„ 4ë¹„íŠ¸ ì •ë°€ë„ë¡œ ë³€í™˜í•˜ê³  ë¡œë“œí•˜ë„ë¡ ì§€ì •í•©ë‹ˆë‹¤.

* **bnb_4bit_use_double_quant = True:** ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ë†’íˆê¸° ìœ„í•´ ì¤‘ì²© ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡  ë° í•™ìŠµí•©ë‹ˆë‹¤.

* **bnb_4bit_quant_type = "nf4":** 4ë¹„íŠ¸ í†µí•©ì—ëŠ” 2ê°€ì§€ ì–‘ìí™” ìœ í˜•ì¸ FP4ì™€ NF4ê°€ ì œê³µëœë‹¤. NF4 dtypeì€ Normal Float 4ë¥¼ ë‚˜íƒ€ë‚´ë©°, QLoRA ë°±ì„œì— ì†Œê°œ ë˜ì–´ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ FP4 ì–‘ìí™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

* **bnb_4bit_compute_dtype = torch.bfloat16:** ê³„ì‚° ì¤‘ ì‚¬ìš©í•  dtypeì„ ë³€ê²½í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ê³„ì‚° dtype. ê¸°ë³¸ì ìœ¼ë¡œ ê³„ì‚° dtypeì€ float32ë¡œ ì„¤ì •ë˜ì–´ ìˆì§€ë§Œ ê³„ì‚° ì†ë„ë¥¼ ë†’íˆê¸° ìœ„í•´ bf16ìœ¼ë¡œ ì„¤ì • ê°€ëŠ¥í•©ë‹ˆë‹¤.


## ê²½ëŸ‰í™” ëª¨ë¸ ë¡œë“œ

huggingfaceì— ìˆëŠ” ëª¨ë¸ idë¥¼ ì§€ì •í•œ ë‹¤ìŒ, ì´ì „ì— ì •ì˜í•œ ì–‘ìí™” êµ¬ì„±ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.


### ëª¨ë¸ ë¡œë“œ



```python
model_id = "kyujinpy/Ko-PlatYi-6B"
# model_id = "TeamUNIVA/Komodo_6B_v3.0.0"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map="auto")
```

<pre>
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
</pre>
<pre>
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
</pre>

```python
# ëª¨ë¸ êµ¬ì¡° í™•ì¸
print(model)
```

<pre>
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(78464, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaSdpaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=512, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=512, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=78464, bias=False)
)
</pre>
## LangChain í™˜ê²½ ì„¤ì •



```python
# LangSmith API KEY
os.environ["LANGCHAIN_API_KEY"] = ""
# Project Name
os.environ["LANGCHAIN_PROJECT"] = ""
# ì¶”ì  ì„¤ì •
os.environ["LANGCHAIN_TRACING_V2"] = "true"
```

## í…ìŠ¤íŠ¸ ìƒì„± íŒŒì´í”„ë¼ì¸ ì„¤ì •



```python
text_generation_pipeline = pipeline(
    model=model,
    tokenizer=tokenizer,
    task="text-generation",
    temperature=0.1, # í¬ê¸°ê°€ í´ ìˆ˜ë¡ ë…ì°½ì ì¸ ëŒ€ë‹µì„ ì‘ì„±
    return_full_text=True,
    max_new_tokens=300,
)
```

## í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •



í”„ë¡¬í”„íŠ¸ ì„¤ì •ì— ë”°ë¼ ì„±ëŠ¥ ì°¨ì´ê°€ ì‹¬í•˜ê¸° ë•Œë¬¸ì— `prompt_template`ì„¤ì •ì„ ë‹¤ì–‘í•˜ê²Œ ë³€ê²½í•˜ê³  ì‹¤í—˜í•´ë´ì•¼ í•©ë‹ˆë‹¤.



```python
prompt_template = """
### [INST]
You are an AI model that answers questions about quality control and defect judgment of key building materials such as sheets, floors, walls, and wall painting.
Find answers to your questions in the content {context}.
Don't use professional words, provide easy-to-understand answers, remove duplicate words, make korean answer.

### QUESTION:
question: {question}

[/INST]
"""

KoPlatYi_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)

# Create prompt from prompt template
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=prompt_template,
)
```

**Prompt Templete Example**



---

Instruction: Answer the question based on your knowledge.   

Please delete the repetition.   

Answer is only korean.   

Here is context to help: {context}



---

You are an AI model with deep QA (Question-Answer) processing capabilities related to plastered via a natural language processing (NLP) based QA (Question-Answer) system.   

Create intelligent responses based on queries for the plastered domain {context}.  



---

You are an AI model with deep QA (Question-Answer) processing capabilities related to plastered via a natural language processing (NLP) based QA (Question-Answer) system.   

Create and readable intelligent responses based on queries to {context}.

Do not use technical words, give easy to understand responses.   



---

You are an AI model that answers questions about quality control and defect judgment of key building materials such as sheets, floors, walls, and wall painting.   

Find answers to your questions in the content {context}.   

Don't use professional words, provide easy-to-understand answers, remove duplicate words, make korean answer.



## LLM Chain ìƒì„±



```python
# Create llm chain
llm_chain = LLMChain(llm=KoPlatYi_llm, prompt=prompt)
```


```python
# CSV LOADER
path = "/content/drive/MyDrive/content/train.csv"
loader = CSVLoader(file_path=path, encoding = 'utf-8')
pages = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
texts = text_splitter.split_documents(pages)

# langchain embedding
# huggingface ì‚¬ìš©
# model_name = "BAAI/bge-base-en-v1.5"
model_name = "jhgan/ko-sbert-nli"
encode_kwargs = {'normalize_embeddings': True}
hf = HuggingFaceEmbeddings(
    model_name=model_name,
    encode_kwargs=encode_kwargs
)

db = FAISS.from_documents(texts, hf)
retriever = db.as_retriever(
                            search_type="similarity",
                            search_kwargs={'k': 5}
                        )
```

<pre>
modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]
</pre>
<pre>
config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]
</pre>
<pre>
README.md:   0%|          | 0.00/4.46k [00:00<?, ?B/s]
</pre>
<pre>
sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]
</pre>
<pre>
config.json:   0%|          | 0.00/620 [00:00<?, ?B/s]
</pre>
<pre>
pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]
</pre>
<pre>
tokenizer_config.json:   0%|          | 0.00/538 [00:00<?, ?B/s]
</pre>
<pre>
vocab.txt:   0%|          | 0.00/248k [00:00<?, ?B/s]
</pre>
<pre>
tokenizer.json:   0%|          | 0.00/495k [00:00<?, ?B/s]
</pre>
<pre>
special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]
</pre>
<pre>
1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]
</pre>

```python
rag_chain = (
 {"context": retriever, "question": RunnablePassthrough()}
    | llm_chain
)
```


```python
warnings.filterwarnings('ignore')
```

## RAG ëª¨ë¸ í™•ì¸



```python
result = rag_chain.invoke("ì–´ë–¤ í™˜ê²½ ìš”ì¸ì´ ëª°ë”© ìˆ˜ì •ì„ ìœ ë°œí•  ìˆ˜ ìˆëŠ”ê°€ìš”? ê·¸ë¦¬ê³  ë°˜ì ì´ ìƒê¸´ì§€ 1ë…„ ì´ë‚´ì¸ í•˜ìì— ëŒ€í•´ ì–´ë–¤ ë³´ìˆ˜ì‘ì—…ì„ í•´ì•¼ í•˜ë‚˜ìš”?")

for i in result['context']:
    print(f"ì£¼ì–´ì§„ ê·¼ê±°: {i.page_content} / ì¶œì²˜: {i.metadata['source']} \n\n")

if result['text'].find('[/INST]') != -1:
  if result['text'].split('[/INST]')[1].find('Answer: ') != -1:
    print(f"ğŸ‘¨ğŸ»â€ğŸ’» {result['text'].split('[/INST]')[1].split('Answer: ')[1]}")
  else:
    print(f"ğŸ‘¨ğŸ»â€ğŸ’» {result['text'].split('[/INST]')[1]}")
else:
  print(f"ğŸ‘¨ğŸ»â€ğŸ’» {result['text']}")
```

<pre>
ì£¼ì–´ì§„ ê·¼ê±°: ë‹µë³€_3: ëª°ë”© ìˆ˜ì •ì€ ë²½ë©´ê³¼ ëª°ë”© ì‚¬ì´ì— ì´ê²©ì´ ìˆê±°ë‚˜, ëª°ë”©ì´ íŒŒì†ëœ ìƒíƒœë¥¼ ê°€ë¦¬í‚¤ë©°, ë””ìì¸ì  ê²°í•¨ìœ¼ë¡œë„ í”íˆ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª°ë”© ìˆ˜ì •ì´ ë°œìƒí•˜ëŠ” ì›ì¸, ì±…ì„ì†Œì¬, ê·¸ë¦¬ê³  í•´ê²° ë°©ë²•ì„ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.  1. í™˜ê²½ì  ìš”ì¸ ì›ì¸: ëª°ë”©ì´ í–‡ë¹›, ìŠµê¸°, í™”í•™ë¬¼ì§ˆ ë˜ëŠ” ë‹¤ë¥¸ ì™¸ë¶€ ìš”ì¸ì— ë…¸ì¶œë  ë•Œ ìˆ˜ì •ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì±…ì„ì†Œì¬: ê±´ë¬¼ì˜ ì†Œìœ ìë‚˜ ê±°ì£¼ìê°€ ì±…ì„ì´ ìˆìŠµë‹ˆë‹¤. í•´ê²° ë°©ë²•: ì£¼ë¡œ ëª°ë”©ì„ êµì²´í•©ë‹ˆë‹¤. ìˆ˜ì •ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë‚´êµ¬ì„±ì´ ê°•í•œ ëª°ë”© ì†Œì¬ë¥¼ ì„ íƒí•˜ëŠ” ê²ƒì´ í•„ìš”í•©ë‹ˆë‹¤.  2. ë‚´êµ¬ì„± ì›ì¸: ëª°ë”© ì¬ë£Œ ìì²´ì˜ ë‚´êµ¬ì„±ì´ ë‚®ì•„ì„œ ìˆ˜ì •ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì±…ì„ì†Œì¬: ì¬ë£Œë¥¼ ì„ íƒí•œ ì‹œê³µì í˜¹ì€ ì‹œê³µì—…ì²´ê°€ ì±…ì„ì´ ìˆìŠµë‹ˆë‹¤. í•´ê²° ë°©ë²•: ë‚´êµ¬ì„±ì´ ê°•í•œ ëª°ë”© ì†Œì¬ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ì •ê¸°ì ì¸ ìœ ì§€ë³´ìˆ˜ë¥¼ í†µí•´ ì´ë¥¼ ë°©ì§€í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.  3. ë¯¸ì„¸í•œ ì˜¤ì—¼ ì›ì¸: ëª°ë”© í‘œë©´ì— ë¨¼ì§€, ì˜¤ì—¼ë¬¼ì´ ìŒ“ì´ë©´ì„œ ì‹œê°ì ìœ¼ë¡œ ìˆ˜ì •ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì±…ì„ì†Œì¬: ê±´ë¬¼ì˜ ê´€ë¦¬ìë‚˜ ì†Œìœ ì, / ì¶œì²˜: /content/drive/MyDrive/content/train.csv 


ì£¼ì–´ì§„ ê·¼ê±°: ï»¿id: TRAIN_336
ì§ˆë¬¸_1: ë°˜ì ì´ ìƒê¸´ì§€ 1ë…„ ì´ë‚´ì¸ í•˜ìëŠ” ë³´ìˆ˜ì‘ì—…ì€ ì–´ë–»ê²Œ í•´?
ì§ˆë¬¸_2: í•˜ìê°€ ë°œìƒí•œ ì§€ 1ë…„ ì´ë‚´ì¸ ê²½ìš° ë³´ìˆ˜ ì‘ì—…ì€ ì–´ë–»ê²Œ ì§„í–‰í•´ì•¼ í•˜ë‚˜ìš”?
category: ë§ˆê°í•˜ì
ë‹µë³€_1: ë²½ì§€ì— ë°˜ì ì´ ìƒê²¼ì„ ë•Œ ìœ ìƒ‰ë°˜ì  ë°œìƒ ì‹œê¸°ê°€ ë²½ì§€ì‹œê³µ í›„ 1ë…„ ì´ë‚´ì¸ ê²½ìš° ë²½ì§€ ì•„ì„¸í†¤ ìš©ì œ í•¨ì¹¨ ë°©ë²•ì„ ì‚¬ìš©í•  ê²½ìš° ìœ ìƒ‰ ë°˜ì ì´ í™•ëŒ€ë˜ê±°ë‚˜ ì‹ ê·œ ì´ì—¼ì´ ë°œìƒí•  ìš°ë ¤ê°€ ìˆìœ¼ë¯€ë¡œ ì´ì—¼ ë°©ì§€ì œë¥¼ ë„í¬í•œ í›„ ê°œì„  ë²½ì§€ë¡œ ì „ë©´ ì¬ì‹œê³µí•´ì•¼í•©ë‹ˆë‹¤.
ë‹µë³€_2: ë²½ì§€ì— ë°˜ì ì´ ìƒê²¼ì„ ë•Œ, ë²½ì§€ ì‹œê³µ í›„ 1ë…„ ì´ë‚´ì— ìœ ìƒ‰ ë°˜ì ì´ ë°œìƒí•œ ê²½ìš° ë³´ìˆ˜ì‘ì—…ì„ ì§„í–‰í•´ì•¼ í•©ë‹ˆë‹¤. ì´ ê²½ìš°, ë²½ì§€ì— ì•„ì„¸í†¤ ìš©ì œë¥¼ ì‚¬ìš©í•˜ì—¬ í•¨ì¹¨í•˜ëŠ” ë°©ë²•ìœ¼ë¡œëŠ” ìœ ìƒ‰ ë°˜ì ì´ í™•ëŒ€ë˜ê±°ë‚˜ ì‹ ê·œ ì´ì—¼ì´ ë°œìƒí•  ìš°ë ¤ê°€ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ì—¼ ë°©ì§€ì œë¥¼ ë„í¬í•œ í›„ì— ê°œì„  ë²½ì§€ë¡œ ì „ë©´ ì¬ì‹œê³µí•´ì£¼ì…”ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë°˜ì ì´ ìƒê²¨ë‚œ ì›ì¸ì„ ì œê±°í•˜ê³ , ë” ë‚˜ì€ ìƒíƒœì˜ ë²½ì§€ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. / ì¶œì²˜: /content/drive/MyDrive/content/train.csv 


ì£¼ì–´ì§„ ê·¼ê±°: ï»¿id: TRAIN_346
ì§ˆë¬¸_1: ë°˜ì ì´ ìƒê¸´ì§€ 1ë…„ ì´ìƒì¸ í•˜ìëŠ” ë³´ìˆ˜ì‘ì—…ì€ ì–´ë–»ê²Œ í•´?
ì§ˆë¬¸_2: ë°˜ì ì´ ìƒê¸´ì§€ 1ë…„ ì´ìƒì¸ í•˜ìë¥¼ ë³´ìˆ˜í•˜ëŠ”ë° ì–´ë–¤ ì ˆì°¨ë¥¼ ê±°ì³ì•¼ í• ê¹Œìš”?
category: ë§ˆê°í•˜ì
ë‹µë³€_1: ë°˜ì ì´ ìƒê¸´ì§€ 1ë…„ ì´ë‚´ì¸ ê²½ìš° ë²½ì§€ ì†ì§€ ë‚´ë¶€ì— ì£¼ì‚¬ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì•„ì„¸í†¤ í•¨ì¹¨ í›„ ê±´ì¡°ì‹œì¼œ ì´ì—¼ëœ ìœ ìƒ‰ë°˜ì ì„ ì¦ë°œ(1íšŒ 20~30ë¶„ ì†Œìš”, 2~3íšŒ ë°˜ë³µ)ì‹œí‚µë‹ˆë‹¤. ì„ê³ ë³´ë“œ ì›ì§€ì™€ ë²½ì§€ ì›ì§€ì— ì¡´ì¬í•˜ëŠ” ìŠ¹í™” ì—¼ë£Œê°€ ë²½ì§€ í‘œë©´ìœ¼ë¡œ ì™„ì „í•˜ê²Œ ì´ì—¼ëœ ê²½ìš° 2~3íšŒ ì•„ì„¸í†¤ ì£¼ì…ì„ ë°˜ë³µí•˜ì—¬ ë°˜ì  ì œê±°ê°€ ê°€ëŠ¥í•˜ë‚˜ ì„ê³ ë³´ë“œ ì›ì§€ì˜ ì—¼ë£Œ í¬ê¸°ê°€ í¬ê±°ë‚˜ ìš©í•´ ì´ì—¼ë˜ì§€ ì•Šì•˜ë˜ ìƒˆë¡œìš´ ì—¼ë£Œê°€ ìš©í•´ë˜ì–´ ë²½ì§€ í‘œë©´ ë°˜ì ì˜ ìƒ‰ì´ ì§™ì–´ì§€ê±°ë‚˜ í¬ê¸°ê°€ ì»¤ì§€ëŠ” ë¶€ìœ„ëŠ” ë²½ì§€ë¥¼ ì¬ì‹œê³µí•´ì•¼ í•©ë‹ˆë‹¤. / ì¶œì²˜: /content/drive/MyDrive/content/train.csv 


ì£¼ì–´ì§„ ê·¼ê±°: ë‹µë³€_3: ëª°ë”©ì´ ìˆ˜ì •ë˜ëŠ” ê²ƒì€ ì£¼ë¡œ í™˜ê²½ì ì¸ ìš”ì¸ì— ì˜í•´ ë°œìƒí•©ë‹ˆë‹¤. í–‡ë¹›, ìŠµê¸°, í™”í•™ë¬¼ì§ˆ ë“±ì˜ ì™¸ë¶€ ìš”ì¸ì— ë…¸ì¶œë  ê²½ìš° ëª°ë”©ì´ ìˆ˜ì •ë  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ìŠµê¸°ê°€ ë§ì€ í™˜ê²½ì´ë‚˜ ê°•í•œ í–‡ë¹›ì´ ë¹„ì¶”ëŠ” ê³³, í™”í•™ë¬¼ì§ˆì´ ì‚¬ìš©ë˜ëŠ” êµ¬ì—­ ë“±ì—ì„œ ëª°ë”© ìˆ˜ì • í˜„ìƒì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ëª°ë”©ì˜ ìˆ˜ëª…ì„ ì—°ì¥í•˜ê¸° ìœ„í•´ì„œëŠ” ì´ëŸ¬í•œ í™˜ê²½ì ì¸ ìš”ì¸ë“¤ì„ ìµœëŒ€í•œ ë°°ì œí•˜ê³  ìœ ì§€ë³´ìˆ˜ë¥¼ ê¾¸ì¤€íˆ í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.
ë‹µë³€_4: ëª°ë”©ì´ ìˆ˜ì •ë˜ëŠ” í™˜ê²½ì  ìš”ì¸ì€ ë‹¤ì–‘í•©ë‹ˆë‹¤. í–‡ë¹›, ìŠµê¸°, í™”í•™ë¬¼ì§ˆ ë˜ëŠ” ë‹¤ë¥¸ ì™¸ë¶€ìš”ì¸ì— ë…¸ì¶œë  ë•Œ ìˆ˜ì •ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ê³ ì˜¨ ë‹¤ìŠµí•œ í™˜ê²½ì´ë‚˜ ê°•í•œ í–‡ë¹›ì— ì¥ê¸°ê°„ ë…¸ì¶œë  ê²½ìš° ëª°ë”©ì´ ìˆ˜ì •ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, íŠ¹ì • í™”í•™ë¬¼ì§ˆì´ë‚˜ ë‹¤ë¥¸ ê°•í•œ ì™¸ë¶€ìš”ì¸ì— ë…¸ì¶œë˜ë©´ ëª°ë”©ì´ ë³€í˜•ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í™˜ê²½ì  ìš”ì¸ë“¤ì„ ìµœëŒ€í•œ ë°°ì œí•˜ì—¬ ëª°ë”©ì„ ë³´ë‹¤ ì˜¤ë«ë™ì•ˆ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê´€ë¦¬í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. / ì¶œì²˜: /content/drive/MyDrive/content/train.csv 


ì£¼ì–´ì§„ ê·¼ê±°: ï»¿id: TRAIN_364
ì§ˆë¬¸_1: ì–´ë–¤ í™˜ê²½ì¡°ê±´ìœ¼ë¡œ ì¸í•´ ëª°ë”©ìˆ˜ì •ì´ ë°œìƒí•  ìˆ˜ ìˆì–´?
ì§ˆë¬¸_2: ì–´ë–¤ í™˜ê²½ì¡°ê±´ì´ ëª°ë”©ìˆ˜ì •ì„ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆëŠ”ì§€ ì•Œë ¤ì£¼ì„¸ìš”.
category: ë§ˆê°í•˜ì
ë‹µë³€_1: ëª°ë”©ì´ í–‡ë¹›, ìŠµê¸°, í™”í•™ë¬¼ì§ˆ ë˜ëŠ” ë‹¤ë¥¸ ì™¸ë¶€ìš”ì¸ì— ë…¸ì¶œë  ë•Œ ìˆ˜ì •ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë‹µë³€_2: ëª°ë”©ì˜ ìˆ˜ì •ì´ ë°œìƒí•˜ëŠ” í™˜ê²½ì¡°ê±´ì€ ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤. ì£¼ë¡œ ëª°ë”©ì´ í–‡ë¹›, ìŠµê¸°, í™”í•™ë¬¼ì§ˆ ë“±ì˜ ì™¸ë¶€ìš”ì¸ì— ë…¸ì¶œë  ë•Œ ìˆ˜ì •ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ í–‡ë¹›ê³¼ ìŠµê¸°ê°€ ê²°í•©í•˜ì—¬ ëª°ë”©ì˜ ìˆ˜ì •ì„ ìœ ë°œí•  ìˆ˜ ìˆìœ¼ë©°, í™”í•™ë¬¼ì§ˆ ë˜ëŠ” ë‹¤ë¥¸ ì™¸ë¶€ ìš”ì¸ì— ì˜í•´ì„œë„ ìˆ˜ì •ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í™˜ê²½ì¡°ê±´ì´ ì ì ˆí•˜ê²Œ ê´€ë¦¬ë˜ì§€ ì•Šìœ¼ë©´ ëª°ë”©ì´ ìˆ˜ì •ë  ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë‹ˆ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. / ì¶œì²˜: /content/drive/MyDrive/content/train.csv 


ğŸ‘¨ğŸ»â€ğŸ’» 
ë‹µë³€_1: ëª°ë”©ì´ í–‡ë¹›, ìŠµê¸°, í™”í•™ë¬¼ì§ˆ ë˜ëŠ” ë‹¤ë¥¸ ì™¸ë¶€ ìš”ì¸ì— ë…¸ì¶œë  ë•Œ ìˆ˜ì •ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ë‹µë³€_2: ë°˜ì ì´ ìƒê¸´ì§€ 1ë…„ ì´ë‚´ì¸ ê²½ìš° ë²½ì§€ ì•„ì„¸í†¤ ìš©ì œ í•¨ì¹¨ ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ ìœ ìƒ‰ ë°˜ì ì´ í™•ëŒ€ë˜ê±°ë‚˜ ì‹ ê·œ ì´ì—¼ì´ ë°œìƒí•  ìš°ë ¤ê°€ ìˆìœ¼ë¯€ë¡œ ì´ì—¼ ë°©ì§€ì œë¥¼ ë„í¬í•œ í›„ ê°œì„  ë²½ì§€ë¡œ ì „ë©´ ì¬ì‹œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
</pre>
## Testì— ì‚¬ìš©í•  CSV íŒŒì¼ ë¡œë“œ ë° ê²°ê³¼ í™•ì¸



```python
# test.csv í˜•íƒœ í™•ì¸
test_csv_path = "/content/drive/MyDrive/content/test.csv"
questions = pd.read_csv(test_csv_path)
# ì§ˆë¬¸ 10ê°œë§Œ í™•ì¸
for i, question in enumerate(questions['ì§ˆë¬¸'][30:40]):
  print(f"ì§ˆë¬¸_{i+1}â“ {question}")
```

<pre>
ì§ˆë¬¸_1â“ ë°˜ë ¤ë™ë¬¼ì„ ìœ„í•œ ê°€êµ¬ë¡œ ë‚®ì€ ë†’ì´ì˜ ê°€êµ¬ì™€ íŒ¨ë¸Œë¦­ ì†Œì¬ì˜ ê°€êµ¬ê°€ ì„ íƒë˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?
ì§ˆë¬¸_2â“ ëª°ë”© ìˆ˜ì •ì„ ì˜ˆë°©í•˜ê¸° ìœ„í•´ ê±´ë¬¼ ë‚´ë¶€ì—ì„œ ì–´ë–¤ ì¢…ë¥˜ì˜ í™˜ê²½ ê´€ë¦¬ê°€ í•„ìš”í•œê°€ìš”?
ì§ˆë¬¸_3â“ KMEW ì„¸ë¼ë¯¹ ì§€ë¶•ì¬ì˜ ë‹¨ì ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”. ë˜í•œ, ì„¸ë¼ë¯¹ íƒ€ì¼ì„ ì‚¬ìš©í•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ë‹¨ì ì€ ë¬´ì—‡ì¸ê°€ìš”?
ì§ˆë¬¸_4â“ ì¤„í¼í‹° ë§ˆê°ì€ ë¬´ì—‡ì¸ê°€ìš”? ë˜í•œ, ì•¡ì²´ë°©ìˆ˜ê³µì‚¬ëŠ” ë¬´ì—‡ì„ í•˜ëŠ” ê²ƒì¸ê°€ìš”?
ì§ˆë¬¸_5â“ í˜ì¸íŠ¸ í•˜ë„ì¬ ì—†ì´ í˜ì¸íŠ¸ë¥¼ ë°”ë¡œ ì¹ í•  ê²½ìš° ì–´ë–¤ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‚˜ìš”?
ì§ˆë¬¸_6â“ ë°”ë‹¥ì¬ê°€ ë‚¨ìœ¼ë©´ ì–´ë–»ê²Œ ì²˜ë¦¬í•˜ëŠ” ê²Œ ì¢‹ì„ê¹Œìš”? ê·¸ë¦¬ê³  ì¥íŒì´ ë‚¨ì„ ë•Œ ì–´ë–»ê²Œ ì²˜ë¦¬í•´ì•¼ í•˜ë‚˜ìš”?
ì§ˆë¬¸_7â“ ë„ë°°ì§€ì— ìƒê¸´ ë°˜ì ì„ ì—†ì• ê¸° ìœ„í•´ ê°€ì¥ íš¨ê³¼ì ì¸ ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?
ì§ˆë¬¸_8â“ ìƒˆì§‘ì¦í›„êµ°ì˜ ì£¼ìš” ì›ì¸ì€ ë¬´ì—‡ì¸ê°€ìš”?
ì§ˆë¬¸_9â“ ë°©ì²­ë„ë£Œ ë„ì¥ ì‘ì—…ì„ ìœ„í•´ í•„ìš”í•œ ë‹¨ê³„ëŠ” ë¬´ì—‡ì¸ê°€ìš”? ë˜í•œ, ì½˜í¬ë¦¬íŠ¸ ë²½ì— êµ¬ë©ì„ ëš«ëŠ” ë°©ë²•ì—ëŠ” ì–´ë–¤ ë„êµ¬ë‚˜ ê¸°ìˆ ì„ ì‚¬ìš©í•´ì•¼ í•˜ë‚˜ìš”?
ì§ˆë¬¸_10â“ ì–´ë–¤ ì¢…ë¥˜ì˜ ì‹¤ë‚´ ì‹ë¬¼ì„ ì„ íƒí•´ì•¼ ì‹ë¬¼ì„ íš¨ê³¼ì ìœ¼ë¡œ ê°€ê¾¸ëŠ” ë° ë„ì›€ì´ ë ê¹Œìš”? ê·¸ë¦¬ê³  ì¸í…Œë¦¬ì–´ì— ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ë„ë°°ì¬ë£ŒëŠ” ë¬´ì—‡ì¸ê°€ìš”?
</pre>

```python
# ë‹µë³€ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
preds = []

# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± ë° ì €ì¥
for question in tqdm(questions['ì§ˆë¬¸']):
    result = rag_chain.invoke(question)

    # ìƒì„±ëœ ë‹µë³€ì„ preds ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
    if result['text'].find('[/INST]') != -1:
      if result['text'].split('[/INST]')[1].find('Answer: ') != -1:
        preds.append(result['text'].split('[/INST]')[1].split('Answer: ')[1])
      else:
        preds.append(result['text'].split('[/INST]')[1])
    else:
      preds.append(result['text'])
```

<pre>
  0%|          | 0/130 [00:00<?, ?it/s]
</pre>

```python
# ê²°ê³¼ 10ê°œë§Œ í™•ì¸
for pred in preds[30:40]:
  print(f"ë‹µë³€â—ï¸ {pred}")
```

<pre>
ë‹µë³€â—ï¸ 
ë‹µë³€: ë°˜ë ¤ë™ë¬¼ì„ ìœ„í•œ ê°€êµ¬ë¡œ ë‚®ì€ ë†’ì´ì˜ ê°€êµ¬ì™€ íŒ¨ë¸Œë¦­ ì†Œì¬ì˜ ê°€êµ¬ê°€ ì„ íƒë˜ëŠ” ì´ìœ ëŠ” ë°˜ë ¤ë™ë¬¼ì˜ ëª©ê³¼ ê´€ì ˆì„ ë³´í˜¸í•˜ê¸° ìœ„í•œ ê²ƒì…ë‹ˆë‹¤. ë‚®ì€ ë†’ì´ì˜ ê°€êµ¬ë¥¼ ì„ íƒí•˜ë©´ ë°˜ë ¤ë™ë¬¼ì˜ ëª©ê³¼ ê´€ì ˆì„ ë³´í˜¸í•  ìˆ˜ ìˆìœ¼ë©°, ê³„ë‹¨ì„ ì¶”ê°€í•˜ë©´ ë‹¤ë¦¬ ê´€ì ˆì„ ë³´í˜¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ íŒ¨ë¸Œë¦­ ì†Œì¬ì˜ ê°€êµ¬ë¥¼ ì„ íƒí•˜ë©´ ê°€ì£½ ì†Œì¬ì˜ ê°€êµ¬ì— ë¹„í•´ ì„¸íƒì´ ìš©ì´í•˜ê³  ìŠ¤í¬ë˜ì¹˜ê°€ ëœ ë°œìƒí•  ìˆ˜ ìˆì–´ ë°˜ë ¤ë™ë¬¼ê³¼ í•¨ê»˜ ë³´ë‹¤ í¸ì•ˆí•œ ê³µê°„ì„ ì¡°ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë‹µë³€â—ï¸ 
ë‹µë³€: ëª°ë”© ìˆ˜ì •ì„ ì˜ˆë°©í•˜ê¸° ìœ„í•´ ê±´ë¬¼ ë‚´ë¶€ì—ì„œ ê³ ìŠµë„ í™˜ê²½ ê´€ë¦¬ë¥¼ í•´ì•¼ í•©ë‹ˆë‹¤.
ë‹µë³€â—ï¸ 

ë‹µë³€: KMEW ì„¸ë¼ë¯¹ ì§€ë¶•ì¬ì˜ ë‹¨ì ì€ ì£¼ë¡œ ìˆ˜ì…í’ˆì´ê¸° ë•Œë¬¸ì— ê°€ê²©ì´ ë¹„ì‹¸ê³  ê¸ˆì† ì§€ë¶•ì— ë¹„í•´ ë¬´ê²ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì„¸ë¼ë¯¹ íƒ€ì¼ì„ ì‚¬ìš©í•  ë•Œ ê³ ë ¤í•´ì•¼ í•  ë‹¨ì ì€ ì£¼ë¡œ ìˆ˜ì…í’ˆì´ê¸° ë•Œë¬¸ì— ê°€ê²©ì´ ë¹„ì‹¸ê³  ê¸ˆì† ì§€ë¶•ì— ë¹„í•´ ë¬´ê²ë‹¤ëŠ” ì ì…ë‹ˆë‹¤.
ë‹µë³€â—ï¸ 

ë‹µë³€: ì•¡ì²´ë°©ìˆ˜ê³µì‚¬ëŠ” ì½˜í¬ë¦¬íŠ¸, ëª¨ë¥´íƒ€ë¥´ ë“±ì˜ í‘œë©´ì— ì•¡ì²´ í˜•íƒœì˜ ë°©ìˆ˜ì œë¥¼ ë„í¬í•˜ê±°ë‚˜ ì¹¨íˆ¬ì‹œí‚¤ê³  ë°©ìˆ˜ì œë¥¼ í˜¼í•©í•œ ëª¨ë¥´íƒ€ë¥´ë¥¼ ë§ë°œë¼ ì¹¨íˆ¬ë¥¼ ë§‰ëŠ” ê³µë²•ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê±´ë¬¼ ë‚´ë¶€ê°€ ìˆ˜ë¶„ì— ì˜í•´ ì†ìƒì„ ì…íˆëŠ” ê²ƒì„ ë°©ì§€í•˜ê³  ë‚´êµ¬ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë‹µë³€â—ï¸ 

ë‹µë³€: í˜ì¸íŠ¸ í•˜ë„ì¬ ì—†ì´ í˜ì¸íŠ¸ë¥¼ ë°”ë¡œ ì¹ í•˜ë©´ ë„ë§‰ì˜ ë‚´êµ¬ì„±ì´ ì•½í•´ì ¸ ì˜¤ë˜ ì§€ë‚˜ì§€ ì•Šì•„ í˜ì¸íŠ¸ê°€ ë²—ê²¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë‹µë³€â—ï¸ 
ë‹µë³€_1: ë°”ë‹¥ì¬ê°€ ë‚¨ìœ¼ë©´ êµ¬ì²­, ì£¼ë¯¼ì„¼í„°ì—ì„œ ìƒí™œíê¸°ë¬¼ ìŠ¤í‹°ì»¤ë¥¼ êµ¬ë§¤í•˜ì—¬ ë°°ì¶œí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¨, ë°”ë‹¥ì¬ì˜ í¬ê¸°ì— ë”°ë¼ ë¹„ìš©ì´ ë‹¤ë¥´ë¯€ë¡œ ì‚¬ì „ì— í¬ê¸°ë¥¼ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.

ë‹µë³€_2: ì¥íŒì´ ë‚¨ì„ ê²½ìš°ì—ëŠ” ìƒí™œíê¸°ë¬¼ ìŠ¤í‹°ì»¤ë¥¼ êµ¬ë§¤í•œ í›„, ì§€ì—­ì˜ êµ¬ì²­ ë˜ëŠ” ì£¼ë¯¼ì„¼í„°ì—ì„œ ë°°ì¶œí•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë§Œ, ì¥íŒì˜ í¬ê¸°ì— ë”°ë¼ ë¹„ìš©ì´ ë‹¬ë¼ì§€ë¯€ë¡œ ì‚¬ì „ì— í¬ê¸°ë¥¼ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì¥íŒì„ ì •í™•íˆ ë°°ì¶œí•˜ëŠ” ê²ƒì€ ì¹œí™˜ê²½ì ì¸ ë°©ë²•ì´ë©°, ì§€ì—­ ì‚¬íšŒì˜ ê¹¨ë—í•œ í™˜ê²½ì„ ìœ ì§€í•˜ëŠ” ë° ê¸°ì—¬í•˜ëŠ” ì¢‹ì€ ë°©ë²•ì…ë‹ˆë‹¤.
ë‹µë³€â—ï¸ 
ë‹µë³€_1: ë„ë°°ì§€ì— ìƒê¸´ ë°˜ì ì„ ì—†ì• ê¸° ìœ„í•´ ê°€ì¥ íš¨ê³¼ì ì¸ ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. ë°˜ì ì´ ìƒê¸´ ë¶€ë¶„ì— ë°”ì¸ë”ë‚˜ ìˆ˜ì„± í”„ë¼ì´ë¨¸ë¥¼ ë„í¬í•˜ì—¬ ì½”íŒ…í•˜ê³  ë‹¤ì‹œ ë„ë°°í•˜ëŠ” ë°©ë²• - ì¥ì : ë¹„êµì  ê°„ë‹¨í•œ í•´ê²°ì±…ìœ¼ë¡œ ë°”ë¡œ ìˆ˜ì •ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. - ë‹¨ì : ì¬ë°œ ê°€ëŠ¥ì„±ì´ ìˆìœ¼ë©°, ì½”íŒ… ë•Œë¬¸ì— ëƒ„ìƒˆê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

2. ë°˜ì ì´ ìƒê¸´ ë¶€ë¶„ì˜ ì„ê³ ë³´ë“œë¥¼ ë¶€ë¶„ì ìœ¼ë¡œ ì˜ë¼ë‚´ê³  ë³´ê°• í›„ ì¬ì‘ì—…í•˜ëŠ” ë°©ë²• - ì¥ì : ìˆ˜ì •ì´ ë°œìƒí•œ ìœ„ì¹˜ì—ì„œì˜ ì¬ë°œ ìœ„í—˜ì´ ë‚®ê³ , ì „ì²´ ì„ê³ ë³´ë“œ ì‘ì—…í•˜ëŠ” ê²ƒì— ë¹„í•´ ì‹œê°„ê³¼ ë¹„ìš©ì´ ì ê²Œ ë“­ë‹ˆë‹¤. - ë‹¨ì : ìˆ˜ì •í•˜ì§€ ì•Šì€ ë¶€ë¶„ì—ì„œ ë‹¤ì‹œ ë°˜ì ì´ ìƒê¸¸ ìš°ë ¤ê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

3. ë²½ë©´ ì „ì²´ ì„ê³ ë³´ë“œë¥¼ ì˜ë¼ë‚´ê³ , ì„ê³ ë³´ë“œë¥¼ êµì²´í•˜ì—¬ ì¬ì‘ì—…í•˜ëŠ” ë°©ë²• - ì¥ì : ê°€ì¥ ê·¼ë³¸ì ì¸ í•´ê²° ë°©ë²•ìœ¼ë¡œ ë°˜ì ì´ ì¬ë°œí•  ê°€ëŠ¥ì„±ì´ ë‚®ì•„ì§‘ë‹ˆë‹¤. - ë‹¨ì : ì‹œê°„ê³¼ ë¹„ìš©ì´ ë§ì´ ì†Œìš”ë˜ë©°, êµì²´ ì‘ì—… ì¤‘ì—ëŠ” ë¨¼ì§€ê°€ ë‚ ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë‹µë³€â—ï¸ 

ë‹µë³€: ìƒˆì§‘ì¦í›„êµ°ì˜ ì£¼ìš” ì›ì¸ì€ íœ˜ë°œì„± ìœ ê¸° í™”í•©ë¬¼ì— ë…¸ì¶œë˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ëŒ€í‘œì ì¸ ë¬¼ì§ˆë¡œëŠ” í¬ë¦„ì•Œë°íˆë“œê°€ ìˆìŠµë‹ˆë‹¤. í¬ë¦„ì•Œë°íˆë“œëŠ” ë°©ë¶€ì œ, ì ‘ì°©ì œ ë“±ì˜ ì›ë£Œë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ìƒˆë¡œ ì§€ì€ ì§‘ì˜ ì‹ ì„ í•œ ëª©ì¬, ìƒˆ ê°€êµ¬, ë§ˆê°ì¬ì—ì„œ ë‚˜ì˜¤ëŠ” íœ˜ë°œì„± ë¬¼ì§ˆë„ ì›ì¸ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¬¼ì§ˆë“¤ì˜ ì¥ê¸°ì  ë…¸ì¶œì€ í˜¸í¡ê¸° ë° í”¼ë¶€ ë¬¸ì œë¥¼ ìœ ë°œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ë‹µë³€â—ï¸ 

ë‹µë³€: ë°©ì²­ë„ë£Œ ë„ì¥ ì‘ì—…ì„ ìˆ˜í–‰í•˜ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:

1. í”¼ë„ë©´ ì •ë¦¬: ë°©ì²­ë„ë£Œ ë„ì¥ì„ ìœ„í•´ í”¼ë„ë©´ì„ ê¹¨ë—í•˜ê²Œ ì •ë¦¬í•©ë‹ˆë‹¤.
2. ë°©ì²­ë„ë£Œ ë„ì¥: ë°©ì²­ë„ë£Œë¥¼ ë„ì¥í•˜ì—¬ ì ì ˆí•œ ë‘ê»˜ë¡œ ë„í¬í•©ë‹ˆë‹¤.
3. ìƒë„ì‘ì—…: ìƒë„ì‘ì—…ì„ í†µí•´ ë§ˆë¬´ë¦¬í•©ë‹ˆë‹¤.

ì½˜í¬ë¦¬íŠ¸ ë²½ì— êµ¬ë©ì„ ëš«ìœ¼ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:

1. ì‚¬ìš©í•  ë„êµ¬ë¥¼ ì„ íƒí•©ë‹ˆë‹¤: ì‘ì€ êµ¬ë©ì˜ ê²½ìš° í–„ë¨¸ ë“œë¦´ê³¼ ëª¨ìŒë“œë¦´ì„ ì‚¬ìš©í•  ìˆ˜ ìˆê³ , ëŒ€í˜• êµ¬ë©ì˜ ê²½ìš° ì½”ì–´ë“œë¦´ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì½”ì–´ë“œë¦´ì´ë‚˜ í•´ë¨¸ ë“œë¦´ê³¼ í•¨ê»˜ ì½˜í¬ë¦¬íŠ¸ êµ¬ë©ì„ ëš«ê¸° ìœ„í•œ ì „ìš© ë“œë¦´ ë¹„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.
2. êµ¬ë©ì„ ëš«ê¸° ì „ì— ì‚¬ìš©í•  ë„êµ¬ê°€ ì í•©í•œì§€ í™•ì¸í•©ë‹ˆë‹¤: ì „ê¸°ì„ , ìˆ˜ë„ê´€, ë‹¤ë¥¸ ë°°ê´€ ë° êµ¬ì¡°ë¬¼ì´ ì—†ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
3. ë³´í˜¸ì¥ë¹„ë¥¼ ì°©ìš©í•˜ê³  ì•ˆì „ ìˆ˜ì¹™ì„ ì¤€ìˆ˜í•˜ì„¸ìš”: ë³´í˜¸ì¥ë¹„ë¥¼ ì°©ìš©í•˜ê³  ì•ˆì „ ìˆ˜ì¹™ì„ ì¤€ìˆ˜í•˜ì—¬ êµ¬ë©ì„ ëš«ì–´ ì£¼ì„¸ìš”.
4. ì½”ì–´ë“œë¦´ì´ë‚˜ í•´ë¨¸ ë“œë¦´ê³¼ í•¨ê»˜ ì½˜í¬ë¦¬íŠ¸ êµ¬ë©ì„ ëš«ê¸° ìœ„í•œ ì „ìš© ë“œë¦´ ë¹„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
5. êµ¬ë©ì„ ëš«ê¸° ì „ì— ì½˜í¬ë¦¬íŠ¸ êµ¬ë©ì„ ëš«ì–´ë„ ê´œì°®ì€ì§€ í™•ì¸í•©ë‹ˆë‹¤: ì „ê¸°ì„ , ìˆ˜ë„ê´€, ë‹¤ë¥¸ ë°°ê´€ ë° êµ¬ì¡°ë¬¼ì´ ì—†ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
ë‹µë³€â—ï¸ 
ë‹µë³€: ì‹¤ë‚´ ì‹ë¬¼ì„ íš¨ê³¼ì ìœ¼ë¡œ ê°€ê¾¸ë ¤ë©´ ìì—°ì¡°ëª…ì´ ìˆëŠ” ê³³ì— ë§ëŠ” ì‹ë¬¼, ê³µê¸° ì •í™”ì‹ë¬¼, ê·¸ë¦¬ê³  íš¨ê³¼ì ì¸ ê´€ë¦¬ë¥¼ ìœ„í•œ ì‹ë¬¼ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. ì¸í…Œë¦¬ì–´ì— ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ë„ë°°ì¬ë£ŒëŠ” ë²½ì§€ì…ë‹ˆë‹¤.
</pre>
## Test Submission



```python
# Test ë°ì´í„°ì…‹ì˜ ëª¨ë“  ì§ˆì˜ì— ëŒ€í•œ ë‹µë³€ìœ¼ë¡œë¶€í„° 512 ì°¨ì›ì˜ Embedding Vector ì¶”ì¶œ
# í‰ê°€ë¥¼ ìœ„í•œ Embedding Vector ì¶”ì¶œì— í™œìš©í•˜ëŠ” ëª¨ë¸ì€ 'distiluse-base-multilingual-cased-v1' ì´ë¯€ë¡œ ë°˜ë“œì‹œ í™•ì¸
from sentence_transformers import SentenceTransformer # SentenceTransformer Version 2.2.2
import pandas as pd

# Embedding Vector ì¶”ì¶œì— í™œìš©í•  ëª¨ë¸(distiluse-base-multilingual-cased-v1) ë¶ˆëŸ¬ì˜¤ê¸°
model = SentenceTransformer('distiluse-base-multilingual-cased-v1')

# ìƒì„±í•œ ëª¨ë“  ì‘ë‹µ(ë‹µë³€)ìœ¼ë¡œë¶€í„° Embedding Vector ì¶”ì¶œ
pred_embeddings = model.encode(preds)
pred_embeddings.shape

# Submissionì€ Dacon ì°¸ê°€ìë¶„ë“¤ì˜ ë‹µë³€ì´ Daconì˜ ë¦¬ë”ë³´ë“œ ì‹œìŠ¤í…œì— ë§ë„ë¡ ì œì¶œë¬¼ì„ ë³€í™˜í•˜ëŠ” ê³¼ì •
# ë³¸ ëŒ€íšŒì—ì„œëŠ” ë‹µë³€ê³¼ ì •ë‹µì„ `distiluse-base-multilingual-cased-v1`ëª¨ë¸ì„ í™œìš©í•˜ì—¬ 512 ì°¨ì›ì˜ ì„ë² ë”© ë²¡í„°ë¡œ ìˆ˜ì¹˜í™”
submit = pd.read_csv('/content/drive/MyDrive/content/sample_submission.csv')
# ì œì¶œ ì–‘ì‹ íŒŒì¼(sample_submission.csv)ì„ í™œìš©í•˜ì—¬ Embedding Vectorë¡œ ë³€í™˜í•œ ê²°ê³¼ë¥¼ ì‚½ì…
submit.iloc[:,1:] = pred_embeddings
submit.head()
```


  <div id="df-4cf1b5f4-f1c4-434f-a6da-89bd65cf374a" class="colab-df-container">
    <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>vec_0</th>
      <th>vec_1</th>
      <th>vec_2</th>
      <th>vec_3</th>
      <th>vec_4</th>
      <th>vec_5</th>
      <th>vec_6</th>
      <th>vec_7</th>
      <th>vec_8</th>
      <th>...</th>
      <th>vec_502</th>
      <th>vec_503</th>
      <th>vec_504</th>
      <th>vec_505</th>
      <th>vec_506</th>
      <th>vec_507</th>
      <th>vec_508</th>
      <th>vec_509</th>
      <th>vec_510</th>
      <th>vec_511</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>TEST_000</td>
      <td>0.035052</td>
      <td>0.061632</td>
      <td>0.048621</td>
      <td>-0.016013</td>
      <td>0.067280</td>
      <td>0.012120</td>
      <td>-0.004570</td>
      <td>0.040479</td>
      <td>0.001867</td>
      <td>...</td>
      <td>-0.037257</td>
      <td>-0.034480</td>
      <td>-0.005678</td>
      <td>-0.028785</td>
      <td>0.027229</td>
      <td>0.014654</td>
      <td>0.038069</td>
      <td>-0.012414</td>
      <td>0.020745</td>
      <td>0.041971</td>
    </tr>
    <tr>
      <th>1</th>
      <td>TEST_001</td>
      <td>-0.000567</td>
      <td>-0.004986</td>
      <td>0.026041</td>
      <td>0.012829</td>
      <td>0.055199</td>
      <td>-0.004876</td>
      <td>-0.014375</td>
      <td>0.004479</td>
      <td>0.008016</td>
      <td>...</td>
      <td>-0.018125</td>
      <td>-0.018701</td>
      <td>0.040230</td>
      <td>-0.038135</td>
      <td>-0.037711</td>
      <td>0.028047</td>
      <td>0.010371</td>
      <td>0.009904</td>
      <td>0.037600</td>
      <td>0.025828</td>
    </tr>
    <tr>
      <th>2</th>
      <td>TEST_002</td>
      <td>0.038843</td>
      <td>-0.004763</td>
      <td>-0.068065</td>
      <td>-0.002894</td>
      <td>0.110650</td>
      <td>-0.041715</td>
      <td>-0.010136</td>
      <td>-0.014742</td>
      <td>0.015165</td>
      <td>...</td>
      <td>-0.030548</td>
      <td>-0.016873</td>
      <td>0.034737</td>
      <td>-0.034864</td>
      <td>0.044394</td>
      <td>-0.005076</td>
      <td>-0.029116</td>
      <td>-0.017710</td>
      <td>0.004980</td>
      <td>0.084817</td>
    </tr>
    <tr>
      <th>3</th>
      <td>TEST_003</td>
      <td>-0.018743</td>
      <td>0.001674</td>
      <td>-0.006065</td>
      <td>0.012363</td>
      <td>0.046875</td>
      <td>-0.080868</td>
      <td>-0.005202</td>
      <td>0.009403</td>
      <td>-0.002780</td>
      <td>...</td>
      <td>0.004894</td>
      <td>-0.015013</td>
      <td>0.063273</td>
      <td>-0.048686</td>
      <td>-0.003577</td>
      <td>0.072423</td>
      <td>-0.018471</td>
      <td>-0.049052</td>
      <td>-0.007638</td>
      <td>0.069612</td>
    </tr>
    <tr>
      <th>4</th>
      <td>TEST_004</td>
      <td>0.006434</td>
      <td>-0.012325</td>
      <td>0.006185</td>
      <td>-0.022150</td>
      <td>0.097693</td>
      <td>-0.030816</td>
      <td>0.037679</td>
      <td>0.080472</td>
      <td>-0.027198</td>
      <td>...</td>
      <td>0.005342</td>
      <td>0.011142</td>
      <td>0.031304</td>
      <td>0.003474</td>
      <td>-0.014165</td>
      <td>-0.017588</td>
      <td>0.016631</td>
      <td>0.027103</td>
      <td>0.017692</td>
      <td>0.066538</td>
    </tr>
  </tbody>
</table>
<p>5 rows Ã— 513 columns</p>
</div>
    <div class="colab-df-buttons">

  <div class="colab-df-container">
    <button class="colab-df-convert" onclick="convertToInteractive('df-4cf1b5f4-f1c4-434f-a6da-89bd65cf374a')"
            title="Convert this dataframe to an interactive table."
            style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px" viewBox="0 -960 960 960">
    <path d="M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z"/>
  </svg>
    </button>

  <style>
    .colab-df-container {
      display:flex;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    .colab-df-buttons div {
      margin-bottom: 4px;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

    <script>
      const buttonEl =
        document.querySelector('#df-4cf1b5f4-f1c4-434f-a6da-89bd65cf374a button.colab-df-convert');
      buttonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';

      async function convertToInteractive(key) {
        const element = document.querySelector('#df-4cf1b5f4-f1c4-434f-a6da-89bd65cf374a');
        const dataTable =
          await google.colab.kernel.invokeFunction('convertToInteractive',
                                                    [key], {});
        if (!dataTable) return;

        const docLinkHtml = 'Like what you see? Visit the ' +
          '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
          + ' to learn more about interactive tables.';
        element.innerHTML = '';
        dataTable['output_type'] = 'display_data';
        await google.colab.output.renderOutput(dataTable, element);
        const docLink = document.createElement('div');
        docLink.innerHTML = docLinkHtml;
        element.appendChild(docLink);
      }
    </script>
  </div>


<div id="df-c14befe5-cd49-411b-9c9f-0acc1801fda0">
  <button class="colab-df-quickchart" onclick="quickchart('df-c14befe5-cd49-411b-9c9f-0acc1801fda0')"
            title="Suggest charts"
            style="display:none;">

<svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
     width="24px">
    <g>
        <path d="M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z"/>
    </g>
</svg>
  </button>

<style>
  .colab-df-quickchart {
      --bg-color: #E8F0FE;
      --fill-color: #1967D2;
      --hover-bg-color: #E2EBFA;
      --hover-fill-color: #174EA6;
      --disabled-fill-color: #AAA;
      --disabled-bg-color: #DDD;
  }

  [theme=dark] .colab-df-quickchart {
      --bg-color: #3B4455;
      --fill-color: #D2E3FC;
      --hover-bg-color: #434B5C;
      --hover-fill-color: #FFFFFF;
      --disabled-bg-color: #3B4455;
      --disabled-fill-color: #666;
  }

  .colab-df-quickchart {
    background-color: var(--bg-color);
    border: none;
    border-radius: 50%;
    cursor: pointer;
    display: none;
    fill: var(--fill-color);
    height: 32px;
    padding: 0;
    width: 32px;
  }

  .colab-df-quickchart:hover {
    background-color: var(--hover-bg-color);
    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);
    fill: var(--button-hover-fill-color);
  }

  .colab-df-quickchart-complete:disabled,
  .colab-df-quickchart-complete:disabled:hover {
    background-color: var(--disabled-bg-color);
    fill: var(--disabled-fill-color);
    box-shadow: none;
  }

  .colab-df-spinner {
    border: 2px solid var(--fill-color);
    border-color: transparent;
    border-bottom-color: var(--fill-color);
    animation:
      spin 1s steps(1) infinite;
  }

  @keyframes spin {
    0% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
      border-left-color: var(--fill-color);
    }
    20% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    30% {
      border-color: transparent;
      border-left-color: var(--fill-color);
      border-top-color: var(--fill-color);
      border-right-color: var(--fill-color);
    }
    40% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-top-color: var(--fill-color);
    }
    60% {
      border-color: transparent;
      border-right-color: var(--fill-color);
    }
    80% {
      border-color: transparent;
      border-right-color: var(--fill-color);
      border-bottom-color: var(--fill-color);
    }
    90% {
      border-color: transparent;
      border-bottom-color: var(--fill-color);
    }
  }
</style>

  <script>
    async function quickchart(key) {
      const quickchartButtonEl =
        document.querySelector('#' + key + ' button');
      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.
      quickchartButtonEl.classList.add('colab-df-spinner');
      try {
        const charts = await google.colab.kernel.invokeFunction(
            'suggestCharts', [key], {});
      } catch (error) {
        console.error('Error during call to suggestCharts:', error);
      }
      quickchartButtonEl.classList.remove('colab-df-spinner');
      quickchartButtonEl.classList.add('colab-df-quickchart-complete');
    }
    (() => {
      let quickchartButtonEl =
        document.querySelector('#df-c14befe5-cd49-411b-9c9f-0acc1801fda0 button');
      quickchartButtonEl.style.display =
        google.colab.kernel.accessAllowed ? 'block' : 'none';
    })();
  </script>
</div>
    </div>
  </div>



```python
# ë¦¬ë”ë³´ë“œ ì œì¶œì„ ìœ„í•œ csvíŒŒì¼ ìƒì„±
submit.to_csv('/content/drive/MyDrive/content/result/RAG_6.csv', index=False)
```

## DACON ë¦¬ë”ë³´ë“œ í™•ì¸

**566íŒ€ ì¤‘ 59ìœ„**   

![hansol_leaderboard](/assets/images/hansol_leaderboard.png)   


---

ì´ë²ˆ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” colabì—ì„œ ë¬´ë£Œë¡œ ì œê³µí•´ì£¼ëŠ” GPU(T4)ë¥¼ ì‚¬ìš©í•˜ì—¬ LLM QA ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ ë³´ì•˜ìŠµë‹ˆë‹¤.   



ë˜í•œ, ëª¨ë¸ ì–‘ìí™”ì™€ RAGë¥¼ ê²°í•©í•œë‹¤ë©´ ë†’ì€ ì„±ëŠ¥ì˜ GPU ì—†ì´ë„ ì›í•˜ëŠ” NLP AI ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.


## References



* [Ko-PlatYi-6B](https://huggingface.co/kyujinpy/Ko-PlatYi-6B) (Ko-PlatYi-6B LLM Model)

* [Komodo_6B_v3.0.0](https://huggingface.co/TeamUNIVA/Komodo_6B_v3.0.0) (TeamUNIVA/Komodo_6B_v3.0.0 LLM Model)

* [Ko-LLM Leaderboard](https://huggingface.co/spaces/upstage/open-ko-llm-leaderboard) (Ko-LLM Leaderboard)

* [HuggingFace Embedding Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) (Embedding Leaderboard)

* [bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5) (Embedding Model)

* [ko-sbert-nli](https://huggingface.co/jhgan/ko-sbert-nli)(Embedding Model)

* [LangChain](https://python.langchain.com/docs/use_cases/question_answering/) (R.A.G)

